{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db394fe12c1bf284",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRw\n",
    "Oi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTExLjE2MSIgaGVpZ2h0PSIxMzQuNjY4\n",
    "IiB2ZXJzaW9uPSIxLjAiPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYyI+PHN0b3Agb2Zmc2V0\n",
    "PSIwIiBzdHlsZT0ic3RvcC1jb2xvcjojYjhiOGI4O3N0b3Atb3BhY2l0eTouNDk4MDM5MjIiLz48\n",
    "c3RvcCBvZmZzZXQ9IjEiIHN0eWxlPSJzdG9wLWNvbG9yOiM3ZjdmN2Y7c3RvcC1vcGFjaXR5OjAi\n",
    "Lz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYSI+PHN0b3Agb2Zmc2V0PSIw\n",
    "IiBzdHlsZT0ic3RvcC1jb2xvcjojZmZkNDNiO3N0b3Atb3BhY2l0eToxIi8+PHN0b3Agb2Zmc2V0\n",
    "PSIxIiBzdHlsZT0ic3RvcC1jb2xvcjojZmZlODczO3N0b3Atb3BhY2l0eToxIi8+PC9saW5lYXJH\n",
    "cmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9ImIiPjxzdG9wIG9mZnNldD0iMCIgc3R5bGU9InN0\n",
    "b3AtY29sb3I6IzVhOWZkNDtzdG9wLW9wYWNpdHk6MSIvPjxzdG9wIG9mZnNldD0iMSIgc3R5bGU9\n",
    "InN0b3AtY29sb3I6IzMwNjk5ODtzdG9wLW9wYWNpdHk6MSIvPjwvbGluZWFyR3JhZGllbnQ+PGxp\n",
    "bmVhckdyYWRpZW50IHhsaW5rOmhyZWY9IiNhIiBpZD0iZSIgeDE9IjE1MC45NjEiIHgyPSIxMTIu\n",
    "MDMxIiB5MT0iMTkyLjM1MiIgeTI9IjEzNy4yNzMiIGdyYWRpZW50VHJhbnNmb3JtPSJtYXRyaXgo\n",
    "LjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRVbml0cz0idXNlclNw\n",
    "YWNlT25Vc2UiLz48bGluZWFyR3JhZGllbnQgeGxpbms6aHJlZj0iI2IiIGlkPSJkIiB4MT0iMjYu\n",
    "NjQ5IiB4Mj0iMTM1LjY2NSIgeTE9IjIwLjYwNCIgeTI9IjExNC4zOTgiIGdyYWRpZW50VHJhbnNm\n",
    "b3JtPSJtYXRyaXgoLjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRV\n",
    "bml0cz0idXNlclNwYWNlT25Vc2UiLz48cmFkaWFsR3JhZGllbnQgeGxpbms6aHJlZj0iI2MiIGlk\n",
    "PSJmIiBjeD0iNjEuNTE5IiBjeT0iMTMyLjI4NiIgcj0iMjkuMDM3IiBmeD0iNjEuNTE5IiBmeT0i\n",
    "MTMyLjI4NiIgZ3JhZGllbnRUcmFuc2Zvcm09Im1hdHJpeCgwIC0uMjM5OTUgMS4wNTQ2NyAwIC04\n",
    "My43IDE0Mi40NjIpIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIvPjwvZGVmcz48cGF0\n",
    "aCBkPSJNNTQuOTE5IDBjLTQuNTg0LjAyMi04Ljk2MS40MTMtMTIuODEzIDEuMDk1QzMwLjc2IDMu\n",
    "MDk5IDI4LjcgNy4yOTUgMjguNyAxNS4wMzJ2MTAuMjE5aDI2LjgxM3YzLjQwNkgxOC42MzhjLTcu\n",
    "NzkzIDAtMTQuNjE2IDQuNjg0LTE2Ljc1IDEzLjU5NC0yLjQ2MiAxMC4yMTMtMi41NzEgMTYuNTg2\n",
    "IDAgMjcuMjUgMS45MDUgNy45MzggNi40NTcgMTMuNTk0IDE0LjI1IDEzLjU5NGg5LjIxOHYtMTIu\n",
    "MjVjMC04Ljg1IDcuNjU3LTE2LjY1NyAxNi43NS0xNi42NTdoMjYuNzgyYzcuNDU0IDAgMTMuNDA2\n",
    "LTYuMTM4IDEzLjQwNi0xMy42MjV2LTI1LjUzYzAtNy4yNjctNi4xMy0xMi43MjYtMTMuNDA2LTEz\n",
    "LjkzOEM2NC4yODIuMzI4IDU5LjUwMi0uMDIgNTQuOTE4IDBtLTE0LjUgOC4yMmMyLjc3IDAgNS4w\n",
    "MzEgMi4yOTggNS4wMzEgNS4xMjUgMCAyLjgxNi0yLjI2MiA1LjA5My01LjAzMSA1LjA5My0yLjc4\n",
    "IDAtNS4wMzEtMi4yNzctNS4wMzEtNS4wOTMgMC0yLjgyNyAyLjI1MS01LjEyNSA1LjAzLTUuMTI1\n",
    "IiBzdHlsZT0iZmlsbDp1cmwoI2QpO2ZpbGwtb3BhY2l0eToxIi8+PHBhdGggZD0iTTg1LjYzOCAy\n",
    "OC42NTd2MTEuOTA2YzAgOS4yMzEtNy44MjYgMTctMTYuNzUgMTdINDIuMTA2Yy03LjMzNiAwLTEz\n",
    "LjQwNiA2LjI3OS0xMy40MDYgMTMuNjI1Vjk2LjcyYzAgNy4yNjYgNi4zMTkgMTEuNTQgMTMuNDA2\n",
    "IDEzLjYyNSA4LjQ4OCAyLjQ5NSAxNi42MjcgMi45NDYgMjYuNzgyIDAgNi43NS0xLjk1NSAxMy40\n",
    "MDYtNS44ODggMTMuNDA2LTEzLjYyNVY4Ni41SDU1LjUxM3YtMy40MDVIOTUuN2M3Ljc5MyAwIDEw\n",
    "LjY5Ni01LjQzNiAxMy40MDYtMTMuNTk0IDIuOC04LjM5OSAyLjY4LTE2LjQ3NiAwLTI3LjI1LTEu\n",
    "OTI1LTcuNzU4LTUuNjA0LTEzLjU5NC0xMy40MDYtMTMuNTk0ek03MC41NzUgOTMuMzEzYzIuNzgg\n",
    "MCA1LjAzMSAyLjI3OCA1LjAzMSA1LjA5NCAwIDIuODI3LTIuMjUxIDUuMTI1LTUuMDMxIDUuMTI1\n",
    "LTIuNzcgMC01LjAzMS0yLjI5OC01LjAzMS01LjEyNSAwLTIuODE2IDIuMjYxLTUuMDk0IDUuMDMx\n",
    "LTUuMDk0IiBzdHlsZT0iZmlsbDp1cmwoI2UpO2ZpbGwtb3BhY2l0eToxIi8+PGVsbGlwc2UgY3g9\n",
    "IjU1LjgxNyIgY3k9IjEyNy43MDEiIHJ4PSIzNS45MzEiIHJ5PSI2Ljk2NyIgc3R5bGU9Im9wYWNp\n",
    "dHk6LjQ0MzgyO2ZpbGw6dXJsKCNmKTtmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6bm9uemVybztz\n",
    "dHJva2U6bm9uZTtzdHJva2Utd2lkdGg6MTUuNDE3NDtzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9r\n",
    "ZS1kYXNoYXJyYXk6bm9uZTtzdHJva2Utb3BhY2l0eToxIi8+PC9zdmc+\n",
    "\"\n",
    "     style=\"display:block;margin:auto;width:10%\" alt=\"Python Logo\"/>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; font-size:200%;\">\n",
    " <b>CIFAR-100 mit Keras</b>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align:center;\">Dr. Matthias HÃ¶lzl</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143799176f63091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88673f0adbf55117",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd55f4e35ced18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81663031958784b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Dictionary mapping CIFAR-100 class indices to names\n",
    "CIFAR100_CLASSES = torchvision.datasets.CIFAR100(\n",
    "    root='./localdata', train=True, download=True).class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1447f5fa43fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_indices(class_names: list[str]) -> list[int]:\n",
    "    \"\"\"Convert class names to CIFAR-100 indices.\"\"\"\n",
    "    return [CIFAR100_CLASSES[name.lower()] for name in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6b5d79bb58568",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class_idx_mapping: dict[int, int] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5ba723241fc16",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def target_transform(label):\n",
    "    return class_idx_mapping.get(label, -1)  # Map to new index, or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7087f7317a6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(class_names: list[str], batch_size: int = 32):\n",
    "    \"\"\"Create training and test data loaders for specified classes.\"\"\"\n",
    "    # Define transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Get class indices for desired classes\n",
    "    class_indices = get_class_indices(class_names)\n",
    "\n",
    "    # Create mapping from original indices to 0..len(class_names)-1\n",
    "    class_idx_mapping = {orig_idx: new_idx for new_idx, orig_idx in enumerate(class_indices)}\n",
    "\n",
    "    # Load full datasets without target_transform\n",
    "    trainset_full = torchvision.datasets.CIFAR100(\n",
    "        root='./localdata', train=True,\n",
    "        download=True,\n",
    "        transform=transform_train\n",
    "    )\n",
    "    testset_full = torchvision.datasets.CIFAR100(\n",
    "        root='./localdata', train=False,\n",
    "        download=True,\n",
    "        transform=transform_test\n",
    "    )\n",
    "\n",
    "    # Filter indices of samples belonging to desired classes\n",
    "    train_indices = [i for i, lbl in enumerate(trainset_full.targets) if lbl in class_indices]\n",
    "    test_indices = [i for i, lbl in enumerate(testset_full.targets) if lbl in class_indices]\n",
    "\n",
    "    # Create subsets of the data\n",
    "    trainset = Subset(trainset_full, train_indices)\n",
    "    testset = Subset(testset_full, test_indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader, test_loader, class_idx_mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd840b162e514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_keras(class_names: list[str], learning_rate: float = 0.0001, fine_tune_at: int = 100):\n",
    "    \"\"\"Create a fine-tunable classifier using Keras 3.\"\"\"\n",
    "    # Load pre-trained EfficientNetV2S with correct input shape\n",
    "    base_model = keras.applications.EfficientNetV2S(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(224, 224, 3),\n",
    "        pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    # Initially freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create new model\n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs)\n",
    "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Fine-tune the base model\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df27b99614e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, loader, class_idx_mapping):\n",
    "        super().__init__()\n",
    "        self.dataset = loader.dataset\n",
    "        self.batch_size = loader.batch_size\n",
    "        self.indices = np.arange(len(self.dataset))  # Local indices for the subset\n",
    "        self.shuffle = True\n",
    "        self.class_idx_mapping = class_idx_mapping\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches per epoch\n",
    "        return max(1, int(np.ceil(len(self.dataset) / self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = min((idx + 1) * self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "\n",
    "        batch_samples = [self.dataset[i] for i in batch_indices]\n",
    "        images = torch.stack([sample[0].permute(1, 2, 0) for sample in batch_samples])\n",
    "        labels = torch.tensor([self.class_idx_mapping[sample[1]] for sample in batch_samples], dtype=torch.long)\n",
    "        return images, labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indices after each epoch if needed\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163944835783bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "        class_names: list[str], epochs: int = 10, batch_size: int = 64, learning_rate: float = 0.0001, fine_tune_at: int = 100):\n",
    "    # Create data loaders and get the class index mapping\n",
    "    train_loader, test_loader, class_idx_mapping = create_data_loaders(class_names, batch_size=batch_size)\n",
    "\n",
    "    # Create model\n",
    "    model = create_classifier_keras(class_names, learning_rate=learning_rate, fine_tune_at=fine_tune_at)\n",
    "\n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        'localdata/best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    # Create data generators with class index mapping\n",
    "    train_gen = DataGenerator(train_loader, class_idx_mapping)\n",
    "    test_gen = DataGenerator(test_loader, class_idx_mapping)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_gen,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "    )\n",
    "\n",
    "    # Load best model\n",
    "    model = keras.models.load_model('localdata/best_model.keras')\n",
    "\n",
    "    # Evaluate model\n",
    "    results = model.evaluate(\n",
    "        test_gen,\n",
    "        steps=len(test_gen)\n",
    "    )\n",
    "    print(f\"\\nTest accuracy: {results[1]:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1a7b0a0dbf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" and False:\n",
    "    # Example 1: Big cats\n",
    "    for fine_tune_at in [100, 50, 0]:\n",
    "        for lr in [1e-4, 1e-3, 1e-5, 1e-2, 1e-6]:\n",
    "            print(f\"\\nTraining classifier for big cats with lr={lr} and ft={fine_tune_at}...\")\n",
    "            model1, history1 = train_and_evaluate([\"leopard\", \"lion\"], learning_rate=lr, fine_tune_at=fine_tune_at)\n",
    "\n",
    "    # Example 2: Furniture\n",
    "    for fine_tune_at in [100, 50, 0]:\n",
    "        for lr in [1e-4, 1e-3, 1e-5, 1e-2, 1e-6]:\n",
    "            print(f\"\\nTraining classifier for furniture with lr={lr} and ft={fine_tune_at}...\")\n",
    "            model2, history2 = train_and_evaluate(\n",
    "                [\"bed\", \"chair\", \"couch\", \"table\", \"wardrobe\"], learning_rate=lr, fine_tune_at=fine_tune_at\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "lang,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
