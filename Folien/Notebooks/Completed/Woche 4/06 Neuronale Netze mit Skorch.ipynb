{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541be9d2179925ea",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRw\n",
    "Oi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTExLjE2MSIgaGVpZ2h0PSIxMzQuNjY4\n",
    "IiB2ZXJzaW9uPSIxLjAiPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYyI+PHN0b3Agb2Zmc2V0\n",
    "PSIwIiBzdHlsZT0ic3RvcC1jb2xvcjojYjhiOGI4O3N0b3Atb3BhY2l0eTouNDk4MDM5MjIiLz48\n",
    "c3RvcCBvZmZzZXQ9IjEiIHN0eWxlPSJzdG9wLWNvbG9yOiM3ZjdmN2Y7c3RvcC1vcGFjaXR5OjAi\n",
    "Lz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYSI+PHN0b3Agb2Zmc2V0PSIw\n",
    "IiBzdHlsZT0ic3RvcC1jb2xvcjojZmZkNDNiO3N0b3Atb3BhY2l0eToxIi8+PHN0b3Agb2Zmc2V0\n",
    "PSIxIiBzdHlsZT0ic3RvcC1jb2xvcjojZmZlODczO3N0b3Atb3BhY2l0eToxIi8+PC9saW5lYXJH\n",
    "cmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9ImIiPjxzdG9wIG9mZnNldD0iMCIgc3R5bGU9InN0\n",
    "b3AtY29sb3I6IzVhOWZkNDtzdG9wLW9wYWNpdHk6MSIvPjxzdG9wIG9mZnNldD0iMSIgc3R5bGU9\n",
    "InN0b3AtY29sb3I6IzMwNjk5ODtzdG9wLW9wYWNpdHk6MSIvPjwvbGluZWFyR3JhZGllbnQ+PGxp\n",
    "bmVhckdyYWRpZW50IHhsaW5rOmhyZWY9IiNhIiBpZD0iZSIgeDE9IjE1MC45NjEiIHgyPSIxMTIu\n",
    "MDMxIiB5MT0iMTkyLjM1MiIgeTI9IjEzNy4yNzMiIGdyYWRpZW50VHJhbnNmb3JtPSJtYXRyaXgo\n",
    "LjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRVbml0cz0idXNlclNw\n",
    "YWNlT25Vc2UiLz48bGluZWFyR3JhZGllbnQgeGxpbms6aHJlZj0iI2IiIGlkPSJkIiB4MT0iMjYu\n",
    "NjQ5IiB4Mj0iMTM1LjY2NSIgeTE9IjIwLjYwNCIgeTI9IjExNC4zOTgiIGdyYWRpZW50VHJhbnNm\n",
    "b3JtPSJtYXRyaXgoLjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRV\n",
    "bml0cz0idXNlclNwYWNlT25Vc2UiLz48cmFkaWFsR3JhZGllbnQgeGxpbms6aHJlZj0iI2MiIGlk\n",
    "PSJmIiBjeD0iNjEuNTE5IiBjeT0iMTMyLjI4NiIgcj0iMjkuMDM3IiBmeD0iNjEuNTE5IiBmeT0i\n",
    "MTMyLjI4NiIgZ3JhZGllbnRUcmFuc2Zvcm09Im1hdHJpeCgwIC0uMjM5OTUgMS4wNTQ2NyAwIC04\n",
    "My43IDE0Mi40NjIpIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIvPjwvZGVmcz48cGF0\n",
    "aCBkPSJNNTQuOTE5IDBjLTQuNTg0LjAyMi04Ljk2MS40MTMtMTIuODEzIDEuMDk1QzMwLjc2IDMu\n",
    "MDk5IDI4LjcgNy4yOTUgMjguNyAxNS4wMzJ2MTAuMjE5aDI2LjgxM3YzLjQwNkgxOC42MzhjLTcu\n",
    "NzkzIDAtMTQuNjE2IDQuNjg0LTE2Ljc1IDEzLjU5NC0yLjQ2MiAxMC4yMTMtMi41NzEgMTYuNTg2\n",
    "IDAgMjcuMjUgMS45MDUgNy45MzggNi40NTcgMTMuNTk0IDE0LjI1IDEzLjU5NGg5LjIxOHYtMTIu\n",
    "MjVjMC04Ljg1IDcuNjU3LTE2LjY1NyAxNi43NS0xNi42NTdoMjYuNzgyYzcuNDU0IDAgMTMuNDA2\n",
    "LTYuMTM4IDEzLjQwNi0xMy42MjV2LTI1LjUzYzAtNy4yNjctNi4xMy0xMi43MjYtMTMuNDA2LTEz\n",
    "LjkzOEM2NC4yODIuMzI4IDU5LjUwMi0uMDIgNTQuOTE4IDBtLTE0LjUgOC4yMmMyLjc3IDAgNS4w\n",
    "MzEgMi4yOTggNS4wMzEgNS4xMjUgMCAyLjgxNi0yLjI2MiA1LjA5My01LjAzMSA1LjA5My0yLjc4\n",
    "IDAtNS4wMzEtMi4yNzctNS4wMzEtNS4wOTMgMC0yLjgyNyAyLjI1MS01LjEyNSA1LjAzLTUuMTI1\n",
    "IiBzdHlsZT0iZmlsbDp1cmwoI2QpO2ZpbGwtb3BhY2l0eToxIi8+PHBhdGggZD0iTTg1LjYzOCAy\n",
    "OC42NTd2MTEuOTA2YzAgOS4yMzEtNy44MjYgMTctMTYuNzUgMTdINDIuMTA2Yy03LjMzNiAwLTEz\n",
    "LjQwNiA2LjI3OS0xMy40MDYgMTMuNjI1Vjk2LjcyYzAgNy4yNjYgNi4zMTkgMTEuNTQgMTMuNDA2\n",
    "IDEzLjYyNSA4LjQ4OCAyLjQ5NSAxNi42MjcgMi45NDYgMjYuNzgyIDAgNi43NS0xLjk1NSAxMy40\n",
    "MDYtNS44ODggMTMuNDA2LTEzLjYyNVY4Ni41SDU1LjUxM3YtMy40MDVIOTUuN2M3Ljc5MyAwIDEw\n",
    "LjY5Ni01LjQzNiAxMy40MDYtMTMuNTk0IDIuOC04LjM5OSAyLjY4LTE2LjQ3NiAwLTI3LjI1LTEu\n",
    "OTI1LTcuNzU4LTUuNjA0LTEzLjU5NC0xMy40MDYtMTMuNTk0ek03MC41NzUgOTMuMzEzYzIuNzgg\n",
    "MCA1LjAzMSAyLjI3OCA1LjAzMSA1LjA5NCAwIDIuODI3LTIuMjUxIDUuMTI1LTUuMDMxIDUuMTI1\n",
    "LTIuNzcgMC01LjAzMS0yLjI5OC01LjAzMS01LjEyNSAwLTIuODE2IDIuMjYxLTUuMDk0IDUuMDMx\n",
    "LTUuMDk0IiBzdHlsZT0iZmlsbDp1cmwoI2UpO2ZpbGwtb3BhY2l0eToxIi8+PGVsbGlwc2UgY3g9\n",
    "IjU1LjgxNyIgY3k9IjEyNy43MDEiIHJ4PSIzNS45MzEiIHJ5PSI2Ljk2NyIgc3R5bGU9Im9wYWNp\n",
    "dHk6LjQ0MzgyO2ZpbGw6dXJsKCNmKTtmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6bm9uemVybztz\n",
    "dHJva2U6bm9uZTtzdHJva2Utd2lkdGg6MTUuNDE3NDtzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9r\n",
    "ZS1kYXNoYXJyYXk6bm9uZTtzdHJva2Utb3BhY2l0eToxIi8+PC9zdmc+\n",
    "\"\n",
    "     style=\"display:block;margin:auto;width:10%\" alt=\"Python Logo\"/>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; font-size:200%;\">\n",
    " <b>Neuronale Netze mit Skorch</b>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align:center;\">Dr. Matthias HÃ¶lzl</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2038e2f0921b25a",
   "metadata": {},
   "source": [
    "\n",
    "# Training Neural Networks using Skorch\n",
    "\n",
    "Skorch is a library for PyTorch that simplifies training in a Scikit-Learn compatible way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b22d50b06afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed25ac6f67e39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e127ab0c335c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = globals().get(\"mnist\") or fetch_openml(\"mnist_784\", version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fda3ac273b7d96",
   "metadata": {},
   "source": [
    "\n",
    "## Transforming the data:\n",
    "\n",
    "- Neural nets generally expect their inputs as `float32` (or possibly `float16`) values.\n",
    "- `skorch` expects classes to be stored as `int64` values.\n",
    "- We change the type of the arrays accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0e658c7153353",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mnist.data.to_numpy().reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "y = mnist.target.to_numpy().astype(np.int64)\n",
    "print(\"Shape of x:\", x.shape, \"- type of x:\", x.dtype)\n",
    "print(\"Shape of y:\", y.shape, \"- type of y:\", y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca4a048f88c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=10_000, random_state=42\n",
    ")\n",
    "print(\"Shape of x train:\", x_train.shape, \"- shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x test:\", x_test.shape, \"- shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a407689abed45",
   "metadata": {},
   "source": [
    "\n",
    "## Normalize / Standardize\n",
    "\n",
    "- Neural networks generally prefer their input to be in the range $(0, 1)$ or $(-1, 1)$.\n",
    "- We need to convert the integer array to floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa3ef18e39c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0, 0, 20:24, 10:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9c731bc136bd6",
   "metadata": {},
   "source": [
    "\n",
    "- Don't use `StandardScaler` for this data, since it will scale each feature\n",
    "  independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ba1c62f8d6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled_with_scaler = scaler.fit_transform(x_train.reshape(-1, 28 * 28)).reshape(\n",
    "    -1, 1, 28, 28\n",
    ")\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "axs[0].imshow(x_train[0, 0], cmap=\"binary\")\n",
    "axs[1].imshow(x_train_scaled_with_scaler[0, 0], cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6940192cab48057",
   "metadata": {},
   "source": [
    "\n",
    "- Since we know the range of the values, we can easily perform the processing\n",
    "  manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fc725ba72ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c807d3cdd4ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0, 0, 20:24, 10:14])\n",
    "plt.imshow(x_train[0, 0], cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94731974d11201",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Implementing the MLP Model using Basic PyTorch Functions\n",
    "\n",
    "- We can implement the model using basic PyTorch tensor operations, without\n",
    "  using `nn.Linear` or other high-level modules.\n",
    "- This approach demonstrates how neural network computations are built from\n",
    "  basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb2ce725d48519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBasic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Manually initialize weights and biases\n",
    "        self.W1 = nn.Parameter(torch.randn(28 * 28, 128) * 0.01)\n",
    "        self.b1 = nn.Parameter(torch.zeros(128))\n",
    "        self.W2 = nn.Parameter(torch.randn(128, 64) * 0.01)\n",
    "        self.b2 = nn.Parameter(torch.zeros(64))\n",
    "        self.W3 = nn.Parameter(torch.randn(64, 10) * 0.01)\n",
    "        self.b3 = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # First layer computations\n",
    "        z1 = torch.matmul(x, self.W1) + self.b1\n",
    "        a1 = torch.relu(z1)\n",
    "        # Second layer computations\n",
    "        z2 = torch.matmul(a1, self.W2) + self.b2\n",
    "        a2 = torch.relu(z2)\n",
    "        # Output layer computations\n",
    "        z3 = torch.matmul(a2, self.W3) + self.b3\n",
    "        # Do not apply Softmax here; CrossEntropyLoss does it internally\n",
    "        return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3fda9cf4dfb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_basic_model = MLPBasic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b64c6c02e540bc",
   "metadata": {},
   "source": [
    "\n",
    "## Implementing a Hand-Written Training Loop\n",
    "\n",
    "- Before using `NeuralNetClassifier`, we can train our model using a manual\n",
    "  training loop.\n",
    "- This demonstrates how backpropagation and optimization steps are performed.\n",
    "- We will use basic PyTorch functions to train the `MLPBasic` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb7f78458efcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dfd5b62b03d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "x_train_tensor = torch.from_numpy(x_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "x_test_tensor = torch.from_numpy(x_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b75257390a284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset and DataLoader for batching\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a87c724d2461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = MLPBasic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcf139668f708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990bc5e59380890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27454deebaf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5d1a42f918e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    y_pred_manual = predicted.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0dc3a3958388cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation metrics\n",
    "def print_scores(y, y_pred):\n",
    "    print(f\"Accuracy:          {accuracy_score(y, y_pred) * 100:.1f}%\")\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy_score(y, y_pred) * 100:.1f}%\")\n",
    "    print(\n",
    "        f\"Precision (macro): {precision_score(y, y_pred, average='macro') * 100:.1f}%\"\n",
    "    )\n",
    "    print(f\"Recall (macro):    {recall_score(y, y_pred, average='macro') * 100:.1f}%\")\n",
    "    print(f\"F1 (macro):        {f1_score(y, y_pred, average='macro') * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc7d7eb4f7da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(y_test, y_pred_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38e4e3b9307de5",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Data Preparation:**\n",
    "  - Convert the NumPy arrays `x_train` and `y_train` to PyTorch tensors.\n",
    "  - Create a `TensorDataset` and `DataLoader` for batching the training data.\n",
    "- **Model Initialization:**\n",
    "  - Instantiate the `MLPBasic` model.\n",
    "- **Loss Function and Optimizer:**\n",
    "  - Use `nn.CrossEntropyLoss` as the loss function.\n",
    "  - Use Stochastic Gradient Descent (SGD) as the optimizer.\n",
    "- **Training Loop:**\n",
    "  - Iterate over the number of epochs.\n",
    "  - For each batch in the DataLoader:\n",
    "    - Zero the gradients using `optimizer.zero_grad()`.\n",
    "    - Perform the forward pass to compute model outputs.\n",
    "    - Compute the loss between the outputs and the target labels.\n",
    "    - Perform backpropagation using `loss.backward()`, which computes the gradients.\n",
    "    - Update the model parameters using `optimizer.step()`.\n",
    "    - Accumulate the running loss for monitoring.\n",
    "  - After each epoch, print the average loss.\n",
    "- **Evaluation:**\n",
    "  - Use `torch.no_grad()` to disable gradient computation during evaluation.\n",
    "  - Compute the outputs on the test set.\n",
    "  - Use `torch.max` to get the predicted classes.\n",
    "  - Convert predictions to NumPy array for evaluation.\n",
    "- **Results:**\n",
    "  - Use the `print_scores` function to display evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f50215c3b4c66e",
   "metadata": {},
   "source": [
    "\n",
    "## Using `NeuralNetClassifier` for Training\n",
    "\n",
    "- `NeuralNetClassifier` is a Scikit-Learn compatible wrapper for PyTorch models.\n",
    "- It simplifies the training process by providing a high-level interface.\n",
    "- We can use it to train the `MLPBasic` model.\n",
    "- We can also move the training data to the GPU for faster computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537229ec7714eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Training on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc636f2e7a1ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_basic_model = MLPBasic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff63c5b1e6a01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_basic_clf = NeuralNetClassifier(\n",
    "    mlp_basic_model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb586fda80d464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_basic_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315fce2c870192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp_basic = mlp_basic_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a660c7dea585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(y_test, y_pred_mlp_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec71d80ab6555e",
   "metadata": {},
   "source": [
    "\n",
    "## Implementing the MLP Model using `nn.Sequential`\n",
    "\n",
    "- `nn.Sequential` allows us to build models by specifying a sequence of\n",
    "  layers.\n",
    "- This is a simpler and more concise way to define models compared to manually\n",
    "  coding the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a908c5eea372c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    "    # For CrossEntropyLoss, do not apply Softmax at the end\n",
    "    # nn.Softmax(dim=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39238a3668fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = NeuralNetClassifier(\n",
    "    mlp_model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9a9d9681ea84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ec262334f9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819939dabbd81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(y_test, y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654df1ec4239967",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Implementing the MLP Model using `nn.Module` (Object-Oriented Version)\n",
    "\n",
    "- By subclassing `nn.Module`, we can define models in an object-oriented way.\n",
    "- This approach provides greater flexibility, which is useful for more complex\n",
    "  models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955250e8941d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(64, 10)\n",
    "        # Note: We do not include Softmax here for CrossEntropyLoss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede4ebf14ec70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_oop = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69370627e92b1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf_oop = NeuralNetClassifier(\n",
    "    mlp_model_oop,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9579af1d2b1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_clf_oop.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df9bab334056aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_mlp_oop = mlp_clf_oop.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698ff23b5b9c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_scores(y_test, y_pred_mlp_oop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5d104075c69d0",
   "metadata": {},
   "source": [
    "\n",
    "## Implementing a Convolutional Neural Network (CNN) using `nn.Sequential`\n",
    "\n",
    "- CNNs are well-suited for image data and can capture spatial hierarchies in\n",
    "  images.\n",
    "- Here, we define a simple CNN with two convolutional layers followed by fully\n",
    "  connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17307a57f83eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, kernel_size=5, stride=(2, 2)),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(10, 20, kernel_size=5, stride=(2, 2)),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(320, 60),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(60, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1aa2ace00929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_clf = NeuralNetClassifier(\n",
    "    conv_model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    max_epochs=20,\n",
    "    lr=0.1,\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97657f3b91205d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0bee1ff9cfe693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_conv = conv_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8b383e3a09c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_scores(y_test, y_pred_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6afe561b88102",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        1, 32, kernel_size=3, padding=1\n",
    "    ),  # Increased filters, smaller kernel, padding\n",
    "    nn.BatchNorm2d(32),  # Added batch normalization\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Increased filters\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),  # Added MaxPooling\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Added another conv layer\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),  # Added MaxPooling\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128 * 7 * 7, 256),  # Adjusted input features based on conv output\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # Added dropout\n",
    "    nn.Linear(256, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0094d456419cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "from skorch.callbacks import LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5a1918065314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LRScheduler(\n",
    "    policy=StepLR,         # The learning rate scheduler class\n",
    "    step_size=5,           # Decay LR every 5 epochs\n",
    "    gamma=0.2,             # Multiply LR by gamma\n",
    "    verbose=True,          # Prints LR updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14eeffc5d2e8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier with the Adam optimizer and a lower learning rate\n",
    "conv_clf = NeuralNetClassifier(\n",
    "    conv_model,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,       # Use the Adam optimizer\n",
    "    max_epochs=20,\n",
    "    lr=0.001,                         # Lower learning rate for Adam\n",
    "    device=device,\n",
    "    callbacks=[lr_scheduler],         # Add the LR scheduler callback\n",
    "    iterator_train__shuffle=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4def26fe168bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e7ed4dac0eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_conv = conv_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970c828ecada329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_scores(y_test, y_pred_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e03423667dbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "lang,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
