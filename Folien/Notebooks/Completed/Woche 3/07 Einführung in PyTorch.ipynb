{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187a9c4748fa6d29",
   "metadata": {
    "lang": "de",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "\n",
    "<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRw\n",
    "Oi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMTExLjE2MSIgaGVpZ2h0PSIxMzQuNjY4\n",
    "IiB2ZXJzaW9uPSIxLjAiPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYyI+PHN0b3Agb2Zmc2V0\n",
    "PSIwIiBzdHlsZT0ic3RvcC1jb2xvcjojYjhiOGI4O3N0b3Atb3BhY2l0eTouNDk4MDM5MjIiLz48\n",
    "c3RvcCBvZmZzZXQ9IjEiIHN0eWxlPSJzdG9wLWNvbG9yOiM3ZjdmN2Y7c3RvcC1vcGFjaXR5OjAi\n",
    "Lz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYSI+PHN0b3Agb2Zmc2V0PSIw\n",
    "IiBzdHlsZT0ic3RvcC1jb2xvcjojZmZkNDNiO3N0b3Atb3BhY2l0eToxIi8+PHN0b3Agb2Zmc2V0\n",
    "PSIxIiBzdHlsZT0ic3RvcC1jb2xvcjojZmZlODczO3N0b3Atb3BhY2l0eToxIi8+PC9saW5lYXJH\n",
    "cmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9ImIiPjxzdG9wIG9mZnNldD0iMCIgc3R5bGU9InN0\n",
    "b3AtY29sb3I6IzVhOWZkNDtzdG9wLW9wYWNpdHk6MSIvPjxzdG9wIG9mZnNldD0iMSIgc3R5bGU9\n",
    "InN0b3AtY29sb3I6IzMwNjk5ODtzdG9wLW9wYWNpdHk6MSIvPjwvbGluZWFyR3JhZGllbnQ+PGxp\n",
    "bmVhckdyYWRpZW50IHhsaW5rOmhyZWY9IiNhIiBpZD0iZSIgeDE9IjE1MC45NjEiIHgyPSIxMTIu\n",
    "MDMxIiB5MT0iMTkyLjM1MiIgeTI9IjEzNy4yNzMiIGdyYWRpZW50VHJhbnNmb3JtPSJtYXRyaXgo\n",
    "LjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRVbml0cz0idXNlclNw\n",
    "YWNlT25Vc2UiLz48bGluZWFyR3JhZGllbnQgeGxpbms6aHJlZj0iI2IiIGlkPSJkIiB4MT0iMjYu\n",
    "NjQ5IiB4Mj0iMTM1LjY2NSIgeTE9IjIwLjYwNCIgeTI9IjExNC4zOTgiIGdyYWRpZW50VHJhbnNm\n",
    "b3JtPSJtYXRyaXgoLjU2MjU0IDAgMCAuNTY3OTcgLTE0Ljk5MSAtMTEuNzAyKSIgZ3JhZGllbnRV\n",
    "bml0cz0idXNlclNwYWNlT25Vc2UiLz48cmFkaWFsR3JhZGllbnQgeGxpbms6aHJlZj0iI2MiIGlk\n",
    "PSJmIiBjeD0iNjEuNTE5IiBjeT0iMTMyLjI4NiIgcj0iMjkuMDM3IiBmeD0iNjEuNTE5IiBmeT0i\n",
    "MTMyLjI4NiIgZ3JhZGllbnRUcmFuc2Zvcm09Im1hdHJpeCgwIC0uMjM5OTUgMS4wNTQ2NyAwIC04\n",
    "My43IDE0Mi40NjIpIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIvPjwvZGVmcz48cGF0\n",
    "aCBkPSJNNTQuOTE5IDBjLTQuNTg0LjAyMi04Ljk2MS40MTMtMTIuODEzIDEuMDk1QzMwLjc2IDMu\n",
    "MDk5IDI4LjcgNy4yOTUgMjguNyAxNS4wMzJ2MTAuMjE5aDI2LjgxM3YzLjQwNkgxOC42MzhjLTcu\n",
    "NzkzIDAtMTQuNjE2IDQuNjg0LTE2Ljc1IDEzLjU5NC0yLjQ2MiAxMC4yMTMtMi41NzEgMTYuNTg2\n",
    "IDAgMjcuMjUgMS45MDUgNy45MzggNi40NTcgMTMuNTk0IDE0LjI1IDEzLjU5NGg5LjIxOHYtMTIu\n",
    "MjVjMC04Ljg1IDcuNjU3LTE2LjY1NyAxNi43NS0xNi42NTdoMjYuNzgyYzcuNDU0IDAgMTMuNDA2\n",
    "LTYuMTM4IDEzLjQwNi0xMy42MjV2LTI1LjUzYzAtNy4yNjctNi4xMy0xMi43MjYtMTMuNDA2LTEz\n",
    "LjkzOEM2NC4yODIuMzI4IDU5LjUwMi0uMDIgNTQuOTE4IDBtLTE0LjUgOC4yMmMyLjc3IDAgNS4w\n",
    "MzEgMi4yOTggNS4wMzEgNS4xMjUgMCAyLjgxNi0yLjI2MiA1LjA5My01LjAzMSA1LjA5My0yLjc4\n",
    "IDAtNS4wMzEtMi4yNzctNS4wMzEtNS4wOTMgMC0yLjgyNyAyLjI1MS01LjEyNSA1LjAzLTUuMTI1\n",
    "IiBzdHlsZT0iZmlsbDp1cmwoI2QpO2ZpbGwtb3BhY2l0eToxIi8+PHBhdGggZD0iTTg1LjYzOCAy\n",
    "OC42NTd2MTEuOTA2YzAgOS4yMzEtNy44MjYgMTctMTYuNzUgMTdINDIuMTA2Yy03LjMzNiAwLTEz\n",
    "LjQwNiA2LjI3OS0xMy40MDYgMTMuNjI1Vjk2LjcyYzAgNy4yNjYgNi4zMTkgMTEuNTQgMTMuNDA2\n",
    "IDEzLjYyNSA4LjQ4OCAyLjQ5NSAxNi42MjcgMi45NDYgMjYuNzgyIDAgNi43NS0xLjk1NSAxMy40\n",
    "MDYtNS44ODggMTMuNDA2LTEzLjYyNVY4Ni41SDU1LjUxM3YtMy40MDVIOTUuN2M3Ljc5MyAwIDEw\n",
    "LjY5Ni01LjQzNiAxMy40MDYtMTMuNTk0IDIuOC04LjM5OSAyLjY4LTE2LjQ3NiAwLTI3LjI1LTEu\n",
    "OTI1LTcuNzU4LTUuNjA0LTEzLjU5NC0xMy40MDYtMTMuNTk0ek03MC41NzUgOTMuMzEzYzIuNzgg\n",
    "MCA1LjAzMSAyLjI3OCA1LjAzMSA1LjA5NCAwIDIuODI3LTIuMjUxIDUuMTI1LTUuMDMxIDUuMTI1\n",
    "LTIuNzcgMC01LjAzMS0yLjI5OC01LjAzMS01LjEyNSAwLTIuODE2IDIuMjYxLTUuMDk0IDUuMDMx\n",
    "LTUuMDk0IiBzdHlsZT0iZmlsbDp1cmwoI2UpO2ZpbGwtb3BhY2l0eToxIi8+PGVsbGlwc2UgY3g9\n",
    "IjU1LjgxNyIgY3k9IjEyNy43MDEiIHJ4PSIzNS45MzEiIHJ5PSI2Ljk2NyIgc3R5bGU9Im9wYWNp\n",
    "dHk6LjQ0MzgyO2ZpbGw6dXJsKCNmKTtmaWxsLW9wYWNpdHk6MTtmaWxsLXJ1bGU6bm9uemVybztz\n",
    "dHJva2U6bm9uZTtzdHJva2Utd2lkdGg6MTUuNDE3NDtzdHJva2UtbWl0ZXJsaW1pdDo0O3N0cm9r\n",
    "ZS1kYXNoYXJyYXk6bm9uZTtzdHJva2Utb3BhY2l0eToxIi8+PC9zdmc+\n",
    "\"\n",
    "     style=\"display:block;margin:auto;width:10%\" alt=\"Python Logo\"/>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center; font-size:200%;\">\n",
    " <b>Einführung in PyTorch</b>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align:center;\">Dr. Matthias Hölzl</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05ef28804a607a",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI\n",
    "Research lab. It provides:\n",
    "\n",
    "- **Tensors**: Similar to NumPy arrays, but with support for GPU acceleration.\n",
    "- **Dynamic Computation Graphs**: Allows flexibility in building and modifying\n",
    "  neural networks during runtime.\n",
    "- **Autograd Module**: Automatic differentiation for Tensors.\n",
    "\n",
    "In this notebook, we will explore PyTorch functionalities and learn how to\n",
    "build neural networks using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864164cfdd88e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6353cb0b35805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b73bcb769943f",
   "metadata": {},
   "source": [
    "\n",
    "## PyTorch Basics\n",
    "\n",
    "### Tensors\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch. They are similar to\n",
    "NumPy arrays but can be used for GPU acceleration and can automatically\n",
    "compute gradients, which are needed to train neural networks.\n",
    "\n",
    "- Parameters: `data`, `dtype`, `device`, `requires_grad`.\n",
    "- `data` can be a list, tuple, NumPy array, scalar, etc.\n",
    "- `dtype` specifies the data type of the tensor.\n",
    "- `device` specifies the device (CPU or GPU) where the tensor will be stored.\n",
    "- `requires_grad` specifies whether the tensor requires gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace69aec574e0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86ff69557ca95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5f3adcc32bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3088a9ba8b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec94b5ae7bae885",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38042c30ff0ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981d943738b545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959418b5ff777be",
   "metadata": {},
   "source": [
    "\n",
    "#### Example for Gradient Computation\n",
    "\n",
    "The gradient of a function is the slope of the function at a given point. It\n",
    "is a generalization of the derivative of a function to multiple dimensions.\n",
    "PyTorch can automatically compute the gradient of a tensor with respect to\n",
    "some scalar value computed from the tensor.\n",
    "\n",
    "The `requires_grad` parameter specifies whether the tensor requires gradient\n",
    "computation. If `requires_grad=True`, PyTorch will track operations on the\n",
    "tensor. The gradient can be accessed using the `.grad` attribute.\n",
    "\n",
    "**Example**: Compute the gradient of $z = \\sum_i x_i^2$ with respect to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608cdcf19d02a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6ceb28ad474fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0d93440a100ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8765a143bf2cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358bd0cf1977f7b",
   "metadata": {},
   "source": [
    "\n",
    "The `backward()` method computes the gradient of the tensor with respect to\n",
    "some scalar value computed from the tensor. It is used to perform\n",
    "backpropagation in neural networks.\n",
    "\n",
    "PyTorch accumulates the gradient for all computations we perform on the\n",
    "tensor, even if we introduce intermediate variables (like `y` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6f494e755f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd492cdd5c8e62d",
   "metadata": {},
   "source": [
    "\n",
    "Once we have called the `backward()` method, we can access the gradient of the\n",
    "tensor with respect to the scalar value computed from the tensor using the\n",
    "`.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc409e2647f38320",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2626a36b5d34e53",
   "metadata": {},
   "source": [
    "\n",
    "The gradient of $z = \\sum_i x_i^2$ with respect to $x_i$ is $2x_i$. Therefore,\n",
    "the gradient of $z$ with respect to $x$ is $[2, 4, 6]$.\n",
    "That is exactly the result we obtained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a0ceec158f4c1",
   "metadata": {},
   "source": [
    "\n",
    "We can also compute the gradient of tensors with higher ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebc77a00151c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.linspace(1.0, 6.0, 6).reshape(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc60536a2ea81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(mat, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeedc99a4b73bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f99a8bdc2af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220dd4112594c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d831fad13312df",
   "metadata": {},
   "source": [
    "\n",
    "The gradient of $z = \\sum_{i,j} 2x_{ij}^3$ with respect to $x_{ij}$ is\n",
    "$2\\cdot 3x_{ij}^2$. Therefore, the gradient of $z = [[1, 2, 3], [4, 5, 6]]$ with\n",
    "respect to $x$ is $[[6, 24, 54], [96, 150, 216]]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4f214e401e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa252d9cb45c42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0919f58f5ea45",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "2 * 3 * mat**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51424a74d0abbd2d",
   "metadata": {},
   "source": [
    "\n",
    "### Converting between NumPy arrays and PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dcc9273ee30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ae97115a94afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ab7c72f55da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46fd8e388e07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array([7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822aa6cc019021af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_from_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b3a5bea907f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_from_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb472b7caddf43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tensor_from_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc3fdc9133ddad",
   "metadata": {},
   "source": [
    "\n",
    "## PyTorch Functions and Modules\n",
    "\n",
    "Let's explore some PyTorch functions used in building neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a09f3a58bdf194",
   "metadata": {},
   "source": [
    "\n",
    "### `torch.randn`\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Returns a tensor filled with random numbers from a normal distribution.\n",
    "- Parameters: `size`, `dtype`, `device`, `requires_grad`.\n",
    "- `size` can be a single integer, a tuple of integers, or passed as several\n",
    "  parameters.\n",
    "- `dtype` specifies the data type of the output tensor.\n",
    "- `device` specifies the device (CPU or GPU) where the tensor will be stored.\n",
    "- `requires_grad` specifies whether the tensor requires gradient computation.\n",
    "  - Gradients are used for backpropagation in neural networks.\n",
    "  - If `requires_grad=True`, PyTorch will track operations on the tensor.\n",
    "  - The gradient can be accessed using the `.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc9e9b4b72fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d114eaa38258c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a419916bbd48c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2, 3, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282e01f46fc92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee35708996f1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2, 3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46791a40da71cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8b9c8e8602a7f",
   "metadata": {},
   "source": [
    "\n",
    "### torch.view()\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Returns a new tensor with the same data but a different shape.\n",
    "- The returned tensor shares the same data with the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c69987ec2fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338ee5fbc040906",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae299fade685f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_view = x.view(-1, 4)  # Flatten the first two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf4f6d89335ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8756f2624a6efb",
   "metadata": {},
   "source": [
    "\n",
    "### torch.max\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Returns the maximum value of all elements in the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eba97ac87fcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 3.0, 2.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504724be76483c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1029b1ae9dc5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4], [4, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ba16c9ecaa4a4",
   "metadata": {},
   "source": [
    "\n",
    "The `dim` parameter specifies the dimension along which the maximum values are\n",
    "computed. If `dim=0`, the maximum values are computed along the rows. If\n",
    "`dim=1`, the maximum values are computed along the columns.\n",
    "\n",
    "The function returns a tuple containing the maximum values and the indices of\n",
    "the maximum values along the specified dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b3e1cf1c23924",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values, indices = torch.max(x, dim=0)\n",
    "print(\"Max values along dim=1:\", max_values)\n",
    "print(\"Indices:\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e70063833b1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values, indices = torch.max(x, dim=1)\n",
    "print(\"Max values along dim=1:\", max_values)\n",
    "print(\"Indices:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d56f8fcae51b3f",
   "metadata": {},
   "source": [
    "\n",
    "We can also pass two tensors with the same shape to the `torch.max` function.\n",
    "The function will return the maximum values and indices from the two tensors\n",
    "for each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf432e91ddeb6ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "torch.max(torch.linspace(-3, 3, steps=7), torch.zeros(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee0e520cec0a9b",
   "metadata": {},
   "source": [
    "\n",
    "### torch.nn.Module\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Base class for all neural network modules.\n",
    "- Your models should subclass `nn.Module`.\n",
    "- Modules can contain other modules, allowing for nested structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843022f1c6a5340",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Simple Neural Network\n",
    "\n",
    "Let's build a simple neural network using PyTorch's `nn.Module` class.\n",
    "\n",
    "The neural network will have the following architecture:\n",
    "\n",
    "- Input layer: 2 neurons\n",
    "- Hidden layer: 3 neurons\n",
    "- Output layer: 1 neuron\n",
    "\n",
    "Since we use matrix multiplication to compute the output of each layer, the\n",
    "number of columns in the input must match the number of rows in the weight\n",
    "matrix. The weight matrix is defined as a tensor with shape `(input_size,\n",
    "output_size)`. The bias vector is defined as a tensor with shape\n",
    "`(output_size,)`. The output of a layer is computed as follows: $$z = xW + b$$\n",
    "\n",
    "Therefore we have\n",
    "\n",
    "- `W1` with shape `(2, 3)` mapping the 2 input neurons to the 3 hidden\n",
    "  neurons.\n",
    "- `b1` with shape `(3,)` adding the bias to the hidden layer.\n",
    "- `W2` with shape `(3, 1)` mapping the 3 hidden neurons to the 1 output\n",
    "  neuron.\n",
    "- `b2` with shape `(1,)` adding the bias to the output layer.\n",
    "\n",
    "The network will use the ReLU activation function for the hidden layer and\n",
    "will output logits for the output layer. The ReLU activation function takes\n",
    "the outputs of a layer and replaces all negative values with 0. Logits are\n",
    "simply the raw scores output by the network. In some cases another activation\n",
    "function (e.g., the sigmoid function) is applied to the logits to obtain\n",
    "values in the range [0, 1], but we will not do this here.\n",
    "\n",
    "The network will be defined as a class that inherits from `nn.Module`. The\n",
    "`__init__` method initializes the network's parameters, and the `forward`\n",
    "method defines the forward pass of the network, i.e., how the input is\n",
    "transformed into the output.\n",
    "\n",
    "The layers of the network will be defined as tensors wrapped `nn.Parameter`\n",
    "objects. By wrapping the tensors in `nn.Parameter` before assigning them to\n",
    "attributes of a `nn.Module`, PyTorch will automatically register them as\n",
    "parameters of the network. This allows PyTorch to track their gradients during\n",
    "backpropagation. We initialize the weights with small random values and the\n",
    "biases with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd67be14fe24d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Parameter(torch.randn(2, 3) * 0.01)\n",
    "        self.b1 = nn.Parameter(torch.zeros(3))\n",
    "        self.W2 = nn.Parameter(torch.randn(3, 1) * 0.01)\n",
    "        self.b2 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = x @ self.W1 + self.b1\n",
    "        a1 = torch.max(z1, torch.zeros_like(z1)) # ReLU activation\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        return z2  # Outputs logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a6e1c129c31cd",
   "metadata": {},
   "source": [
    "\n",
    "Let's create an instance of the `SimpleNet` class and pass some input data\n",
    "through the network.\n",
    "\n",
    "We set the random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0567bd29490e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "simple_net = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6338843277ac826",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe78a1810f6ec31",
   "metadata": {},
   "source": [
    "\n",
    "The `forward` method of the network defines the forward pass of the network,\n",
    "i.e., how the input is transformed into the output. The `forward` method is\n",
    "called when we pass input data to the network. In the simplest case, we pass a\n",
    "tensor of shape `(input_size,)` to the network, where `input_size` is the\n",
    "number of input neurons, in our case 2.\n",
    "\n",
    "The `forward` method then computes the output of the network by applying the\n",
    "weights and biases of the network to the input data. The output of the network\n",
    "is the logits, i.e., the raw scores output by the network.\n",
    "\n",
    "Of course, the output of the network represents a random function, as we have\n",
    "initialized the weights randomly and not yet trained the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071c84fd07f89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net.forward(torch.ones(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0559e4650e9c357",
   "metadata": {},
   "source": [
    "\n",
    "Typically, we don't call the `forward` method directly. Instead, we use the\n",
    "network as a function, which internally calls the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e665c8c6b230f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net(torch.ones(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e92dbe6e46495",
   "metadata": {},
   "source": [
    "\n",
    "Typically we don't want to compute the output of the network for a single\n",
    "input. Instead, we want to pass a batch of inputs to the network.\n",
    "\n",
    "This is particularly important when training the network, as we want to update\n",
    "the weights based on the gradients computed from multiple inputs. Therefore,\n",
    "the input to the network is usually a tensor of shape `(batch_size,\n",
    "input_size)`, where `batch_size` is the number of inputs in the batch.\n",
    "\n",
    "The Broadcasting rules in PyTorch allow us to pass a tensor of this shape to\n",
    "the network. In that case, we get a tensor of shape `(batch_size,\n",
    "output_size)` as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eaecb08adacbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net(torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7e5541e158a29",
   "metadata": {},
   "source": [
    "\n",
    "The network accumulates the gradients of the parameters during the forward\n",
    "pass. Typically we provide our training data multiple times to the network,\n",
    "since a single pass through the training data is not sufficient to learn the\n",
    "parameters of the network. Each pass through the training data is called an\n",
    "epoch.\n",
    "\n",
    "After each epoch, we update the parameters of the network based on the\n",
    "gradients computed during the forward pass. This process is called\n",
    "backpropagation.\n",
    "\n",
    "After each epoch we need to reset the gradients of the parameters to zero,\n",
    "since PyTorch accumulates the gradients by default. We can do this by calling\n",
    "the `zero_grad` method of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c182e95f201044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9deaa43c48bac5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "### Training a Simple Neural Network\n",
    "\n",
    "Let's train the simple neural network we defined above. We will generate some\n",
    "training data and define a loss function to measure the difference between the\n",
    "output of the network and the expected output.\n",
    "\n",
    "The training data will be a tensor of shape `(n_samples, 2)`, where\n",
    "`n_samples` is the number of samples in the training data. The expected output\n",
    "will be a tensor of shape `(n_samples, 1)`.\n",
    "\n",
    "The `generate_data` function generates random training data and expected\n",
    "output for the network. The expected output is the mean of the input data\n",
    "along the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b9450c1ee788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=5) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = torch.rand((n_samples, 2), dtype=torch.float32) * 2 - 1\n",
    "    y = X.mean(dim=1).reshape(-1, 1)\n",
    "    n_samples = X.shape[0]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53828c30fb373bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20146e4b30b10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09cf5241768939",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5670ae2e4e96f8",
   "metadata": {},
   "source": [
    "\n",
    "If we apply our network to the training data, we expect a very bad\n",
    "performance, as the network has not been trained yet.\n",
    "\n",
    "How can we measure the performance of the network in a way that we can then\n",
    "use to improve its performance?\n",
    "\n",
    "There are many ways to measure the performance of a network. One common way is\n",
    "to use the mean absolute error (MAE) as the loss function. The MAE is the mean\n",
    "of the absolute differences between the predicted and true values.\n",
    "\n",
    "The MAE is defined as: $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i -\n",
    "\\hat{y}_i|$$\n",
    "\n",
    "where $y_i$ is the true value and $\\hat{y}_i$ is the predicted value for the\n",
    "$i$-th sample.\n",
    "\n",
    "To compute the MAE, for our network, we run the network on input for which we\n",
    "know the true output. We then compute the absolute difference between the\n",
    "predicted and true output and take the mean of these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb95e9e29aa7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = simple_net(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61cb82ae72bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9c0cfc1053f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (result - y_train).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3d395146d3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647fc47073fb189",
   "metadata": {},
   "source": [
    "\n",
    "The loss is very high, as expected, since the network has not been trained\n",
    "yet.\n",
    "\n",
    "To train the network, we need to update the parameters of the network based on\n",
    "the gradients computed during the forward pass. We can do this by calling the\n",
    "`backward` method of the loss tensor. The `backward` method computes the\n",
    "gradients of the loss with respect to the parameters of the network.\n",
    "\n",
    "After computing the gradients, we can update the parameters of the network\n",
    "using an optimization algorithm. For now, we will update the parameters\n",
    "manually using a learning rate of 0.1, i.e., we will subtract the gradient\n",
    "times the learning rate from the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debb7230232bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf6f014c1c04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net.W1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd54d3a6161ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net.b1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60c55d57a1f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for param in simple_net.parameters():\n",
    "        param -= 0.1 * param.grad\n",
    "    print(simple_net(X_train)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813de5067a2254e",
   "metadata": {},
   "source": [
    "\n",
    "The output of the network has changed after updating the parameters. We can\n",
    "now compute the loss again and check if it has decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ba6c0ef11b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(simple_net(X_train) - y_train).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9262e89509d8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "The loss has decreased, which is a good sign, although the decrease was\n",
    "perhaps not as large as we would have liked, even though we used 500 samples.\n",
    "\n",
    "This shows one of the challenges of training neural networks: we need quite a\n",
    "lot of data to train the network effectively. In practice, we typically use\n",
    "thousands or even millions of samples to train a neural network.\n",
    "\n",
    "We can repeat the process we just applied multiple times to further decrease\n",
    "the loss: Send data through the network, adjust the parameters to (hopefully)\n",
    "reduce the loss, validate that we have achieved an improvement. This process\n",
    "is called training the network.\n",
    "\n",
    "We typically train the network for multiple epochs, i.e., we pass the training\n",
    "data to the network multiple times. After each epoch, we update the parameters\n",
    "of the network based on the gradients computed during the forward pass.\n",
    "\n",
    "It is important to reset the gradients of the parameters to zero after each\n",
    "epoch, as PyTorch accumulates the gradients by default and we don't want to\n",
    "accumulate the gradients from multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed76b170227452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(model, num_epochs=100, num_samples=5000, learning_rate=1e-4):\n",
    "    X, y = generate_data(num_samples)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        result = model(X)\n",
    "        loss = (result - y).abs().mean()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "        model.zero_grad()\n",
    "        if epoch % (num_epochs / 10) == 0:\n",
    "            print(f\"Epoch {epoch}, loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968e2c1364cc084",
   "metadata": {},
   "source": [
    "\n",
    "The function `print_results` generates new test data and prints the expected\n",
    "output, the predicted output, and the error for the first five samples. It\n",
    "also prints the mean error over all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45691e57972b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results():\n",
    "    X, y = generate_data(500)\n",
    "    predicted = simple_net(X).detach()\n",
    "    error = np.abs(predicted - y)\n",
    "    values = (\n",
    "        np.concatenate(\n",
    "            [\n",
    "                y,\n",
    "                predicted,\n",
    "                error,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "    print(\"Expected | Predicted | Error\")\n",
    "    print(values[:5])\n",
    "    print(f\"Mean error: {error.mean():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5508684cba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cddc99d0db10ed1",
   "metadata": {},
   "source": [
    "\n",
    "Let's train the network for 400 epochs and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b430f0cab126e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "simple_net = SimpleNet()\n",
    "\n",
    "run_training_loop(simple_net, num_epochs=400, learning_rate=0.15)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380363ceb74dd104",
   "metadata": {},
   "source": [
    "\n",
    "The mean error has decreased significantly after training the network for 400\n",
    "epochs. The network has learned to predict the mean of the input data much\n",
    "better than tha untrained net.\n",
    "\n",
    "We see that the loss during each training epoch decreased noticably. This is a\n",
    "good sign that the network is learning.\n",
    "\n",
    "However, if you try to train the network for many more epochs with the\n",
    "training rate of 0.15, you will notice that it does not become better, and\n",
    "even starts to get worse. This is because the learning rate, while initially\n",
    "successful in improving the net, is becoming too high, and the network starts\n",
    "to overshoot the minimum of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbea97fcc40ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop(simple_net, num_epochs=1000, learning_rate=0.15)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5cb258dd0204b",
   "metadata": {},
   "source": [
    "\n",
    "To avoid this problem, we can reduce the learning rate. A smaller learning rate\n",
    "means that the network updates the parameters more slowly, which can help to\n",
    "avoid overshooting the minimum of the loss function.\n",
    "\n",
    "Often we start the training with a relatively high learning rate and then\n",
    "decrease it during training. This is called learning rate scheduling.\n",
    "\n",
    "Let's try a simple learning rate scheduling: We start with a learning rate of\n",
    "0.15 for 400 epochs, then decrease it to 0.1 for 250 epochs, then to 0.03 for\n",
    "100 epochs, and so on. I've found these values by trial and error, but there\n",
    "are more sophisticated methods to find good learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ddb6004172d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "simple_net = SimpleNet()\n",
    "\n",
    "run_training_loop(simple_net, num_epochs=400, learning_rate=0.15)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318417c847ef0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop(simple_net, num_epochs=200, learning_rate=0.03)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39ccb7f5e6ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop(simple_net, num_epochs=100, learning_rate=0.01)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f4a69987cf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop(simple_net, num_epochs=200, learning_rate=3e-3)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465de3d6487d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop(simple_net, num_epochs=200, learning_rate=5e-4)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e5734c6fa2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop(simple_net, num_epochs=200, learning_rate=1e-6)\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f088df18f9166",
   "metadata": {},
   "source": [
    "\n",
    "By introducing this simple learning rate schedule, we were able to train the\n",
    "network to predict the mean of the input data almost perfectly: The mean error\n",
    "of the final network on the test data is less than 1e-4, a rather satisfying\n",
    "result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4440b017719ff2",
   "metadata": {},
   "source": [
    "\n",
    "Let's now turn to another method of defining neural networks in PyTorch: the\n",
    "`torch.nn.Sequential` class, and let's also look at a few more PyTorch\n",
    "functions and modules.\n",
    "\n",
    "### `torch.nn.Flatten` and `torch.flatten`\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Flattens a contiguous range of dimensions into a tensor.\n",
    "- Commonly used to flatten inputs before feeding them into fully connected\n",
    "  layers.\n",
    "- `nn.Flatten` is a module, while `torch.flatten` is a functional interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284fbf46d52d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290618eae27afe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3449be97a17796",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b4871fa7314cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_tensor = flatten(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc8a60dc4aea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa9629921c8962",
   "metadata": {},
   "source": [
    "\n",
    "### torch.nn.Linear\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "- Parameters: `in_features`, `out_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7268f5da19451",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(3, 2)\n",
    "input_tensor = torch.randn(1, 3)\n",
    "output_tensor = linear_layer(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2cb63881e0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbc0ddf4f54e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029dff6d6625fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9d70a298c2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5f6ac9dc404de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor @ linear_layer.weight.T + linear_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfb5d8a5ed8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d31cae30df8e8",
   "metadata": {},
   "source": [
    "\n",
    "### torch.nn.ReLU and torch.relu\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- The ReLU activation function introduces non-linearity.\n",
    "- `nn.ReLU` is a module, while `torch.relu` is a functional interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a12bf934276c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308a671e6ec0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9ec88d406b669",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdcbf87e0621edb",
   "metadata": {},
   "source": [
    "\n",
    "**Visualization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648e1c12d85b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_module = nn.ReLU()\n",
    "x = torch.linspace(-5, 5, steps=100)\n",
    "y = relu_module(x)\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "plt.title(\"ReLU Activation Function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e78f7500e7e6f3",
   "metadata": {},
   "source": [
    "\n",
    "### torch.nn.Softmax and torch.softmax\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Applies the Softmax function to a tensor.\n",
    "- Converts logits into probabilities that sum to 1.\n",
    "\n",
    "**Visualization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06be1b304a192cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_module = nn.Softmax(dim=0)\n",
    "logits = torch.tensor([1.0, 2.0, 3.0])\n",
    "probabilities = softmax_module(logits)\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Probabilities after Softmax:\", probabilities)\n",
    "print(\"Sum of probabilities:\", probabilities.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee9991126a61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.linspace(-2, 2, steps=10)\n",
    "probabilities = torch.softmax(logits, dim=0)\n",
    "plt.bar(logits.numpy(), probabilities.numpy())\n",
    "plt.title(\"Softmax Function\")\n",
    "plt.xlabel(\"Logits\")\n",
    "plt.ylabel(\"Probabilities\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0dc4bc4a46141",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.linspace(-2, 2, steps=10)\n",
    "probabilities = torch.softmax(logits, dim=0)\n",
    "cumulative_probabilities = torch.cumsum(probabilities, dim=0)\n",
    "plt.bar(logits.numpy(), cumulative_probabilities.numpy())\n",
    "plt.title(\"Cumulative Distribution Function\")\n",
    "plt.xlabel(\"Logits\")\n",
    "plt.ylabel(\"Cumulative Probabilities\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c61f23a472de4",
   "metadata": {},
   "source": [
    "\n",
    "### torch.nn.Conv2d\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Applies a 2D convolution over an input signal composed of several input planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6757509e4bf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Conv2d layer\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "input_image = torch.randn(1, 1, 5, 5)  # Batch size, channels, height, width\n",
    "output_image = conv_layer(input_image)\n",
    "print(\"Input shape:\", input_image.shape)\n",
    "print(\"Output shape:\", output_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f2083618a9a66",
   "metadata": {},
   "source": [
    "\n",
    "### torch.nn.Sequential\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- A sequential container to build neural networks.\n",
    "- Modules will be added in the order they are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b0bab8eb62b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using nn.Sequential\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10)\n",
    ")\n",
    "print(sequential_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885f1dafbc31bd2",
   "metadata": {},
   "source": [
    "\n",
    "### torch.nn.CrossEntropyLoss\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Combines `nn.LogSoftmax()` and `nn.NLLLoss()` in a single class.\n",
    "- Useful for multi-class classification problems.\n",
    "\n",
    "**Important Concept**:\n",
    "\n",
    "**Cross-Entropy Loss** measures the performance of a classification model\n",
    "whose output is a probability value between 0 and 1. It increases as the\n",
    "predicted probability diverges from the actual label.\n",
    "\n",
    "In the context of multi-class classification:\n",
    "\n",
    "- The model outputs a vector of raw scores (logits) for each class.\n",
    "- These scores are converted to probabilities using the softmax function.\n",
    "- The cross-entropy loss is then calculated between these probabilities and\n",
    "  the one-hot encoded true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f626638b0a51ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2d7670238fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[0.5, 1.5, 0.3, 0.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a74c1698f74c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([1])  # True class index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f3414b022d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747af831191a8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ca8fa35735e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = torch.softmax(logits.squeeze(), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049414f595c0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a6ddbd72b6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(sm[target]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22530bf40536f6d4",
   "metadata": {},
   "source": [
    "\n",
    "Very confident correct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4a0ea31b2a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[0.1, 10.0, 0.1, 0.1]])\n",
    "target = torch.tensor([1])\n",
    "loss = criterion(logits, target)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec559815784114",
   "metadata": {},
   "source": [
    "\n",
    "Very confident incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776706a5ae8a9c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([[0.1, 0.1, 10.0, 0.1]])\n",
    "target = torch.tensor([1])\n",
    "loss = criterion(logits, target)\n",
    "print(loss.item())  # Output: 9.999954223632812"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c6d26bb90eb13",
   "metadata": {},
   "source": [
    "\n",
    "Uncertain prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7011b0659e83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[2.0, 2.1, 1.9, 2.05]])\n",
    "target = torch.tensor([1])\n",
    "loss = criterion(logits, target)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb16c496456927",
   "metadata": {},
   "source": [
    "\n",
    "Multi-sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30823a9a3efc783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[1.0, 2.0, 0.5, 0.8]])\n",
    "target = torch.tensor([1])\n",
    "loss1 = criterion(logits, target)\n",
    "print(loss1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34a78ffd636915",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[0.2, 0.3, 0.9, 1.2]])\n",
    "target = torch.tensor([3])\n",
    "loss2 = criterion(logits, target)\n",
    "print(loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2c74894896085",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[2.0, 1.0, 3.0, 0.5]])\n",
    "target = torch.tensor([2])\n",
    "loss3 = criterion(logits, target)\n",
    "print(loss3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cf1171338d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss1 + loss2 + loss3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c0d8bf6c48c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[1.0, 2.0, 0.5, 0.8],\n",
    "                       [0.2, 0.3, 0.9, 1.2],\n",
    "                       [2.0, 1.0, 3.0, 0.5]])\n",
    "target = torch.tensor([1, 3, 2])\n",
    "loss = criterion(logits, target)\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7050466a8a15c62",
   "metadata": {},
   "source": [
    "\n",
    "### torch.optim.SGD\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Implements stochastic gradient descent optimization algorithm.\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398ba90d2f397eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(2, 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a901c8ac35d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([w], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2455cd97b621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (w**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6629171afd2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165757f8d6fe777",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "print(\"Updated parameters:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf2b382e12c244",
   "metadata": {},
   "source": [
    "\n",
    "### torch.utils.data.TensorDataset and DataLoader\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- `TensorDataset`: Wraps tensors to create a dataset.\n",
    "- `DataLoader`: Provides an iterable over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2bd10c44a2fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.randn(100, 3)\n",
    "y_tensor = torch.randint(0, 2, (100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b890741f12e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9532f4b316123",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6556a39286d15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the DataLoader\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}: Data shape {data.shape}, Target shape {target.shape}\")\n",
    "    break  # Just show one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e2e6e1d85e149",
   "metadata": {},
   "source": [
    "\n",
    "### torch.no_grad()\n",
    "\n",
    "**Description**:\n",
    "\n",
    "- Context-manager that disables gradient calculation.\n",
    "- Reduces memory consumption during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0874c024b0c0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238976e6687960b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (x * 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad95d3678052ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a3fa9aa92e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce320773973be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = (x * 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2806aa63ac9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136df19d1e21c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.backward()  # This will raise an error"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "lang,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
